# 9장 웹 크롤러 설계
### 크롤러
* 검색 엔진 인덱싱 : google bot
* [web archiving](https://archive.org/) : 장기보관
* web mining
* web monitoring
* 복잡도는 처리해야 하는 데이터 규모에 따라 달라짐

## 1단계 - 요구사항
### 동작
1. 목록의 URL의 웹 페이지를 다운로드
2. 웹페이지에서 URL 및 데이터 추출
3. 추출된 URL을 다운로드 목록에 추가

### 세부 요구사항
* 규모 확장 가능
* 안정성
  * 웹은 함정으로 가득
* POLITENESS
  * 요청이 너무 많으면 블럭될 수도 있다
* 확장성
  * 새로운 형태 지원하기 쉬워야 함

### 규모 추정
* 월 10억 페이지 조회, QPS : 400/s
* peak : QPS * 2
* 웹 페이지 평균 500 k로 가정
* 월 500TB, 5년 기준 30PB

## 2단계 - 개략적 설계
* 시작 URL
* FIFO queue에 시작 URL에서 획득한 url을 넣는다
  * url Filter : invalid 한 url을 필터링  
* url로 부터 HTML을 다운로드
    * 다운받기 위해 url에 대응하는 ip 획득
* html을 파싱
  * 중복인지 확인
  * url 추출
  * url을 queue에 저장
* html 저장소에 저장
* url 저장소 : 이미 방문한 url 제외위한 저장

## 3단계 -상세 설계

### DFS vs BFS
* 일반적으로 BFS 사용, 깊이를 알 수 없기 때문
  * 같은 페이지를 참조하게 될 가능성이 높다
  * URL간의 우선순위가 없는데, 실제로는 그렇지 않다
### 미수집 URL 저장소
* politeness
  * 크롤러는 DoS로 간주 될 수 있다
  * 기본적으로 동일 웹 사이트에는 한번에 하나의 페이지만 요청하도록 해야 한다
  * queue router를 이용해 같은 호스트는 같은 큐로 가도록 보장
  * mapping table로 큐와 호스트의 매핑을 관리한다
  * FIFO queue로 같은 호스트에서 한번에 하나씩만 나오도록 보장한다
  * queue selector로 queue를 round robin cjfl gksek
  * worker thread는 전달된 URL을 처리한다
* 우선순위
  * QUEUE에 넣기전에 Prioritizer로로 우선순위를 결정한다
    * pagerank, traffic, 갱신 빈도 등..
* 재수집
  * 중요하거나, 갱신이 잦은 페이지인 경우 재수집이 필요하다
  * 우선 순위가 높은 경우 재수집을 자주하도록 한다

### HTML Downloader
* robots.txt
  * 크롤러에게 허용하거나 허용하지 않을 규칙이 있음
* 성능 최적화
  * 분산 크롤링
  * dns 캐싱
  * 지역 분산
  * timeout은 짧게
* 안정성
  * 부하 분산을 위한 5장의 안정 해시
  * 크롤링 상태 및 수집 데이터 저장
  * 예외 처리
  * 데이터 검증
* 확장성
  * 진화하지 않는 시스템은 없다, 새로운 형태의 컨텐츠를 쉽게 지원할 수 있도록 설계했는지 확인해보자
* 문제 있는 콘텐츠 감지 및 회피
  * 중복 : 해시나 체크섭으로 중복을 탐지
  * 거미 덫 : 크롤러를 무한 루프에 빠지게 설계한 웹 페이지
  * 데이터 노이즈 : 광고나 , 스팸 등
## 4단계 - 마무리
* server-side rendering 
  * 웹 페이지를 js나 ajax로 생성한 경우 html 자체는 아무것도 없다.
  * 서버에서 렌더링 하여 파싱한다
* 원치않는 페이지 필터
  * 크롤링의 자원은 유한하기 때문에 스팸성인 페이지를 필터아웃 하자
* 데이터베이스 다중화 및 샤딩
  * 데이터 계층의 가용성 확장성 안정성이 향상된다
* 수평적 규모 확장성
  * 대규모 크롤링을 위해 서버가 stateless이게 만드는것이 중요하다
* 가용성, 일관성, 안정성
  * 성공적 대형 시스템을 위해 필수로 고려해야함
* 데이터 분석 솔루션
  * 시스템의 세밀한 조정을 위해서는 데이터와 분석 결과가 필수적이다